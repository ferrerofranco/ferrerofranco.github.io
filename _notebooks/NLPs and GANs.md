---
title: "202101 - NLPs and GANs"
excerpt: "Playing around with GANs and NLPs"
collection: notebooks
author_profile: True
---

# How it started
Through a friend of mine I recently came across a [couple](https://colab.research.google.com/drive/1J7KhWkyRpyMkREwIgk8cwi89JiyJ6Yjl?usp=sharing) of [notebooks](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing) posted by [@advadnoun](https://twitter.com/advadnoun) in which he used [CLIP](https://openai.com/blog/clip/) with both [BigGAN](https://arxiv.org/abs/1809.11096) and [SIREN](https://vsitzmann.github.io/siren/) to create images with a user input phrase.  
After having some fun with it and trying to make my friends guess what the picture represented I thought  *well... there are plenty of models that do the inverse, right? You input an image and they try to describe it* (it's called "Image captioning"). *We could input a phrase, translate it into an image, and then translate it back and see what we get. Kind of like between different languages but with images.* 

That sounded cool to me, so I did it.  
  
While in the process I also thought of a second question *would the image caption part of the pipeline understand real life images of that original input phrase better than the ones generated by another network?*  
We'll see!  

# How it went
I worked with two models. The first one uses VGG16 and the second one uses Xception as a photo feature extractor, and they both use RNN and a FF layers as sequence processor and decoding respectively. Here are the links to the notebooks on the [former](https://colab.research.google.com/drive/1CEVUmu8giZuoNc7Qy17cvLcIK9m4ddFA?usp=sharing) and the [latter](https://colab.research.google.com/drive/16kWT9I57T8ZkEeJ5RCFjUa7dFqgusYuY?usp=sharing)
  
After training them they were both subjected to a [BLEU](https://en.wikipedia.org/wiki/BLEU) score analysis, here are the scores:  

| BLEU-n | VGG16 | Xception |
| ------ | ----- | -------- |
| BLEU-1 | 0.558444 | 0.347332 |
| BLEU-2 | 0.299974 | 0.189991 |
| BLEU-3 | 0.197463 | 0.130232 |
| BLEU-4 | 0.086908 | 0.055207 |  

According to Jason Brownlee, as a reference, these are ballpark BLEU scores for a decent model:

BLEU-1: **0.401** to **0.578**.  
BLEU-2: **0.176** to **0.390**.  
BLEU-3: **0.099** to **0.260**.  
BLEU-4: **0.059** to **0.170**.  

As you can see, the VGG model seems to be on the upper spectrum of the reference scores, while the Xception model is on the lower, directly failing the BLEU-1 score.

Now for the results of the experiment! (I haven't uploaded the notebook that runs both models at the same time and gets the BLEU scores, but I might if requested)  
Note: Only the macaw image was generated with BigGAN, all others were generated with SIREN.


| Generating phrase | Generated | Real life |
| ----------------- | --------- | --------- |
| **a cat eating a mouse** | ![cat eating a mouse A](https://ferrerofranco.github.io/images/01-A-cat%20eating%20a%20mouse.png) | ![cat eating a mouse B](https://ferrerofranco.github.io/images/01-B-cat%20eating%20a%20mouse.jpg) |
| - | VGG16: **young boy is playing with ball in the air** <br>Xception: **two boys are playing on the edge of carnival ride** | VGG16: **dog is running through the grass** <br>Xception: **dog is jumping over fallen tree** |
| **a dog playing with a kid** | ![dog playing with a kid A](https://ferrerofranco.github.io/images/02-A-dog%20playing%20with%20a%20kid.png) | ![dog playing with a kid B](https://ferrerofranco.github.io/images/02-B-dog%20playing%20with%20a%20kid.jpg) |
| - | VGG16: **two girls are playing in the water** <br>Xception: **man in black shirt and backpack is standing in front of glass store** | VGG16: **young boy is playing with ball in the air** <br>Xception: **dog is jumping over fallen tree** |
| **a macaw flying over a forest** | ![macaw flying over a forest A](https://ferrerofranco.github.io/images/03-A-macaw%20flying%20over%20a%20forest.png) | ![macaw flying over a forest B](https://ferrerofranco.github.io/images/03-B-macaw%20flying%20over%20a%20forest.jpg) |
| - | VGG16: **the man is sitting on the street with his arms in the air** <br>Xception: **man in red shirt and blue pants is walking on the sidewalk** | VGG16: **man in red shirt is standing on the street** <br>Xception: **man in red shirt is standing on bleachers in front of leafless trees** |
| **a plane flying in the sky** | ![plane flying in the sky A](https://ferrerofranco.github.io/images/04-A-plane%20flying%20in%20the%20sky.png) | ![plane flying in the sky B](https://ferrerofranco.github.io/images/04-B-plane%20flying%20in%20the%20sky.jpg) |
| - | VGG16: **dog is running through the grass** <br>Xception: **man in red shirt is cooking gun** | VGG16: **dog is running through the air to catch ball** <br>Xception: **dog is jumping over home** |
| **a woman and a man in the beach** | ![woman and a man in the beach A](https://ferrerofranco.github.io/images/05-A-woman%20and%20a%20man%20in%20the%20beach.png) | ![woman and a man in the beach B](https://ferrerofranco.github.io/images/05-B-woman%20and%20a%20man%20in%20the%20beach.jpg) |
| - | VGG16: **two girls are playing in the water** <br>Xception: **boy is jumping into the water** | VGG16: **two girls are playing in the water** <br>Xception: **two people are standing on the beach looking at something** |
| **batman fighting crime** | ![batman fighting crime A](https://ferrerofranco.github.io/images/06-A-batman%20fighting%20crime.png) | ![batman fighting crime B](https://ferrerofranco.github.io/images/06-B-batman%20fighting%20crime.jpg) |
| - | VGG16: **young boy is playing with ball in the air** <br>Xception: **man in black shirt and backpack is standing in front of skyscraper** | VGG16: **young boy is playing with ball in the air** <br>Xception: **man in red shirt is standing on the edge of bicycle** |
| **an octopus on coral reef** | ![octopus on coral reef A](https://ferrerofranco.github.io/images/07-A-octopus%20on%20coral%20reef.png) | ![octopus on coral reef B](https://ferrerofranco.github.io/images/07-B-octopus%20on%20coral%20reef.jpg) |
| - | VGG16: **man in red shirt is standing on the street** <br>Xception: **two people are standing in front of house** | VGG16: **man in red shirt is standing on the street** <br>Xception: **man in red shirt is standing on the edge of dock** |
| **a palm tree on an island** | ![palm tree on an island A](https://ferrerofranco.github.io/images/08-A-palm%20tree%20on%20an%20island.png) | ![palm tree on an island B](https://ferrerofranco.github.io/images/08-B-palm%20tree%20on%20an%20island.jpg) |
| - | VGG16: **dog is running through the grass** <br>Xception: **man in red shirt is standing on the edge of dock** | VGG16: **man in red shirt is standing on the street** <br>Xception: **man in red shirt is walking along the beach** |
| **a cat sleeping on a couch** | ![cat sleeping on couch A](https://ferrerofranco.github.io/images/09-A-cat%20sleeping%20on%20couch.png) | ![cat sleeping on couch B](https://ferrerofranco.github.io/images/09-B-cat%20sleeping%20on%20couch.jpg) |
| - | VGG16: **young boy is playing with ball in the air** <br>Xception: **man in white shirt and jeans is petting machine** | VGG16: **young boy is playing with ball in the air** <br>Xception: **dog is running through the grass** |
| **two guys playing chess** | ![two guys playing chess A](https://ferrerofranco.github.io/images/10-A-two%20guys%20playing%20chess.png) | ![two guys playing chess B](https://ferrerofranco.github.io/images/10-B-two%20guys%20playing%20chess.jpg) |
| - | VGG16: **young boy is playing with ball in the air** <br>Xception: **man in black shirt and backpack is standing in front of crowd** | VGG16: **young boy is playing with ball in the air** <br>Xception: **man with his hair and red shirt is sitting in front of an art display** |

# Conclusions
 I know this wasn't a very good test of the hypothesis, the models are a bit basic and I didn't spend any time fine tuning/modifying them, the sample size is tiny, photos have different resolutions and the batman one is from a videogame (it was surprisingly difficult to find a "real life" picture instead of comics and drawings). But this was mostly for fun! (and to learn) So it was ok.  
   
Despite the BLEU scores, I actually like the Xception model's predictions better, was specially surprised with "two people are standing on the beach looking at something", that was pretty close!  
You can see they both kind of default to some phrase when they are not sure what's going on, like "young boy is playing with ball" for VGG and "man in [colour] shirt standing in front of/the edge of [thing]" for Xception.  
  
Going back to the original questions that started all this,  
- "can we go from text to image back to test and get good results?", I'd say the answer is no, not yet, at least not with these models. The generated images are more like art than real images, OpenAI has recently released [DALL-E](https://openai.com/blog/dall-e/), which is far more promissing and might be better for this experiment, but I don't have access to the code yet.
- "can the model understand real life images of that original input phrase better than the ones generated by another network?" Debatable, in this case I'd say yes by a tiny margin.

Thats all! hope you found it interesting, thanks for reading!  

If you'd like to contact me please do so [here](mailto:info@ferrerofranco.com)